"""
Q-Learning Optimis√© pour DonatelloPyzza
Entra√Ænement continu jusqu'√† l'optimisation parfaite (moins de cases possible)
"""

import sys
import os
import random
import time
import pickle
from typing import Dict, Tuple, List, Set
from collections import defaultdict

# Configuration du chemin d'acc√®s au module parent
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.abspath(os.path.join(current_dir, ".."))
sys.path.insert(0, parent_dir)

from donatellopyzza import RLGame, Action, Feedback


class OptimizedQLearningAgent:
    """
    Agent Q-Learning optimis√© pour trouver le chemin le plus court vers la pizza
    
    Fonctionnalit√©s:
        - Entra√Ænement continu jusqu'√† la d√©couverte de la pizza
        - D√©tection du chemin optimal
        - Affichage en temps r√©el des performances
        - Arr√™t automatique quand l'optimal est atteint
        - Visualisation du chemin parcouru
    """
    
    def __init__(
        self,
        learning_rate: float = 0.1,
        discount_factor: float = 0.9,
        epsilon: float = 0.2,
        epsilon_decay: float = 0.995,
        epsilon_min: float = 0.01
    ):
        """
        Initialise l'agent Q-Learning optimis√©
        
        Args:
            learning_rate: Taux d'apprentissage (alpha)
            discount_factor: Facteur de r√©duction (gamma)
            epsilon: Taux d'exploration initial
            epsilon_decay: Facteur de d√©croissance d'epsilon
            epsilon_min: Valeur minimale d'epsilon
        """
        # Hyperparam√®tres
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # Q-table: dictionnaire d'√©tats vers dictionnaire d'actions-valeurs
        self.q_table: Dict[str, Dict[int, float]] = defaultdict(lambda: defaultdict(float))
        
        # Actions disponibles
        self.actions: List[Action] = [
            Action.MOVE_FORWARD,
            Action.TURN_LEFT,
            Action.TURN_RIGHT,
            Action.TOUCH
        ]
        
        # Configuration des r√©compenses optimis√©es
        self.rewards_config = {
            'pizza_found': 1000.0,       # R√©compense tr√®s √©lev√©e pour la pizza
            'pizza_touched': 100.0,      # R√©compense pour toucher la pizza
            'collision': -50.0,          # P√©nalit√© forte pour collision
            'step_cost': -1.0,           # Co√ªt par √©tape (encourage l'efficacit√©)
            'new_cell': 10.0,            # Bonus pour exploration
            'revisit_penalty': -5.0,     # P√©nalit√© pour revisites
            'efficiency_bonus': 50.0     # Bonus pour chemin court
        }
        
        # Statistiques d'optimisation
        self.best_steps = float('inf')
        self.best_reward = float('-inf')
        self.consecutive_optimal = 0
        self.optimal_threshold = 5  # Nombre de succ√®s cons√©cutifs pour consid√©rer optimal
        
        # Historique des performances
        self.performance_history = []
        self.episode_count = 0
        
        # Statistiques d'entra√Ænement
        self.reset_episode_stats()
        
    def reset_episode_stats(self):
        """R√©initialise les statistiques de l'√©pisode en cours"""
        self.visited_cells: Set[Tuple[int, int]] = set()
        self.cell_visit_count: Dict[Tuple[int, int], int] = defaultdict(int)
        self.episode_path: List[Tuple[int, int]] = []
        
    def get_state_representation(
        self,
        position: Tuple[int, int],
        orientation: int,
        last_feedback: Feedback = None
    ) -> str:
        """
        G√©n√®re une repr√©sentation d'√©tat unique
        
        Args:
            position: Position actuelle (x, y)
            orientation: Orientation actuelle (0-3)
            last_feedback: Dernier feedback re√ßu
            
        Returns:
            Cha√Æne de caract√®res repr√©sentant l'√©tat
        """
        state = f"pos_{position[0]}_{position[1]}_ori_{orientation}"
        
        if last_feedback == Feedback.TOUCHED_WALL:
            state += "_wall"
        elif last_feedback == Feedback.TOUCHED_PIZZA:
            state += "_pizza"
        elif last_feedback == Feedback.TOUCHED_NOTHING:
            state += "_empty"
            
        return state
    
    def calculate_reward(
        self,
        feedback: Feedback,
        current_position: Tuple[int, int],
        steps: int
    ) -> float:
        """
        Calcule la r√©compense optimis√©e pour l'efficacit√©
        
        Args:
            feedback: Feedback re√ßu de l'environnement
            current_position: Position actuelle
            steps: Nombre d'√©tapes effectu√©es
            
        Returns:
            Valeur de la r√©compense
        """
        reward = 0.0
        
        # R√©compenses bas√©es sur le feedback
        if feedback == Feedback.MOVED_ON_PIZZA:
            reward = self.rewards_config['pizza_found']
            # Bonus d'efficacit√© si le chemin est court
            if steps < self.best_steps:
                reward += self.rewards_config['efficiency_bonus']
        elif feedback == Feedback.TOUCHED_PIZZA:
            reward = self.rewards_config['pizza_touched']
        elif feedback == Feedback.COLLISION:
            reward = self.rewards_config['collision']
        else:
            reward = self.rewards_config['step_cost']
        
        # Gestion de l'exploration des cellules
        if current_position not in self.visited_cells:
            reward += self.rewards_config['new_cell']
            self.visited_cells.add(current_position)
            self.cell_visit_count[current_position] = 1
        else:
            self.cell_visit_count[current_position] += 1
            visits = self.cell_visit_count[current_position]
            if visits > 1:
                revisit_penalty = self.rewards_config['revisit_penalty'] * (visits - 1)
                reward += revisit_penalty
        
        return reward
    
    def choose_action(self, state: str) -> Action:
        """
        S√©lectionne une action selon la politique epsilon-greedy
        
        Args:
            state: √âtat actuel
            
        Returns:
            Action √† ex√©cuter
        """
        # Exploration: action al√©atoire
        if random.random() < self.epsilon:
            return random.choice(self.actions)
        
        # Exploitation: meilleure action connue
        if state in self.q_table and self.q_table[state]:
            best_action_value = max(self.q_table[state].items(), key=lambda x: x[1])
            action_index = best_action_value[0]
            
            for action in self.actions:
                if int(action.value) == action_index:
                    return action
        
        # Si aucune information, action al√©atoire
        return random.choice(self.actions)
    
    def update_q_value(
        self,
        state: str,
        action: Action,
        reward: float,
        next_state: str
    ):
        """
        Met √† jour la Q-table selon l'√©quation de Bellman
        
        Args:
            state: √âtat actuel
            action: Action effectu√©e
            reward: R√©compense re√ßue
            next_state: Nouvel √©tat
        """
        action_key = int(action.value)
        current_q = self.q_table[state][action_key]
        
        # Calcul de la valeur maximale du prochain √©tat
        if next_state in self.q_table and self.q_table[next_state]:
            max_next_q = max(self.q_table[next_state].values())
        else:
            max_next_q = 0.0
        
        # √âquation de Bellman: Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
        td_target = reward + self.discount_factor * max_next_q
        td_error = td_target - current_q
        new_q = current_q + self.learning_rate * td_error
        
        self.q_table[state][action_key] = new_q
    
    def decay_exploration(self):
        """R√©duit le taux d'exploration epsilon"""
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
    
    def is_optimal_performance(self, steps: int, reward: float) -> bool:
        """
        D√©termine si la performance est optimale
        
        Args:
            steps: Nombre d'√©tapes effectu√©es
            reward: R√©compense totale obtenue
            
        Returns:
            True si la performance est optimale
        """
        # Mise √† jour des meilleures performances
        if steps < self.best_steps:
            self.best_steps = steps
            self.consecutive_optimal = 0
        
        if reward > self.best_reward:
            self.best_reward = reward
        
        # V√©rification si la performance est optimale
        if steps <= self.best_steps and reward >= self.best_reward * 0.9:
            self.consecutive_optimal += 1
            return self.consecutive_optimal >= self.optimal_threshold
        else:
            self.consecutive_optimal = 0
            return False
    
    def visualize_path(self, game: RLGame, path: List[Tuple[int, int]]):
        """
        Visualise le chemin parcouru par la tortue
        
        Args:
            game: Instance du jeu
            path: Liste des positions parcourues
        """
        try:
            # Marque les cellules visit√©es
            for position in path:
                visit_count = self.cell_visit_count[position]
                # Couleur en fonction du nombre de visites
                if visit_count == 1:
                    color = "lightblue"  # Premi√®re visite
                elif visit_count == 2:
                    color = "yellow"     # Deuxi√®me visite
                else:
                    color = "orange"     # Visites multiples
                
                # Marque la cellule (si la m√©thode existe)
                if hasattr(game, 'mark_cell'):
                    game.mark_cell(position, color)
        except Exception as e:
            # Ignore les erreurs de visualisation
            pass
    
    def train_episode_optimized(
        self,
        game: RLGame,
        turtle,
        show_gui: bool = True,
        verbose: bool = True
    ) -> Tuple[float, int, bool, bool]:
        """
        Entra√Æne l'agent sur un √©pisode avec optimisation
        L'√©pisode continue jusqu'√† ce que la pizza soit trouv√©e
        
        Args:
            game: Instance du jeu
            turtle: Tortue √† contr√¥ler
            show_gui: Afficher l'interface graphique
            verbose: Afficher les d√©tails de chaque √©tape
            
        Returns:
            Tuple (r√©compense totale, nombre d'√©tapes, succ√®s, performance optimale)
        """
        self.reset_episode_stats()
        
        total_reward = 0.0
        steps = 0
        step_delay = 0.01  # D√©lai fixe entre les √©tapes
        
        # √âtat initial
        current_position = game.getTurtlePosition(turtle)
        current_orientation = game.getTurtleOrientation(turtle)
        current_state = self.get_state_representation(current_position, current_orientation)
        
        # Enregistrement du chemin
        self.episode_path.append(current_position)
        
        # Noms pour l'affichage
        action_names = {
            "MOVE_FORWARD": "AVANCER",
            "TURN_LEFT": "TOURNER_GAUCHE", 
            "TURN_RIGHT": "TOURNER_DROITE",
            "TOUCH": "TOUCHER"
        }
        
        feedback_names = {
            "MOVED_ON_PIZZA": "PIZZA TROUV√âE!",
            "TOUCHED_PIZZA": "PIZZA TOUCH√âE!",
            "COLLISION": "COLLISION!",
            "TOUCHED_WALL": "MUR TOUCH√â",
            "TOUCHED_NOTHING": "RIEN TOUCH√â",
            "MOVED": "D√âPLACEMENT OK"
        }
        
        # Boucle principale: continue jusqu'√† trouver la pizza
        while True:
            steps += 1
            
            # S√©lection et ex√©cution de l'action
            action = self.choose_action(current_state)
            feedback, _ = turtle.execute(action)
            
            # Calcul de la r√©compense
            new_position = game.getTurtlePosition(turtle)
            reward = self.calculate_reward(feedback, new_position, steps)
            total_reward += reward
            
            # Enregistrement du chemin
            if new_position != current_position:
                self.episode_path.append(new_position)
            
            # Affichage des informations de l'√©tape
            if verbose:
                feedback_str = feedback_names.get(str(feedback), str(feedback))
                print(f"√âtape {steps:4d}: {action_names.get(str(action), str(action)):16s} | "
                      f"Pos: {new_position} | "
                      f"R√©c.: {reward:7.1f} | "
                      f"{feedback_str}")
            
            # Nouvel √©tat
            new_orientation = game.getTurtleOrientation(turtle)
            next_state = self.get_state_representation(
                new_position,
                new_orientation,
                feedback
            )
            
            # Mise √† jour de la Q-table (apprentissage par renforcement)
            self.update_q_value(current_state, action, reward, next_state)
            
            # D√©lai pour la visualisation
            if show_gui and step_delay > 0:
                time.sleep(step_delay)
            
            # V√©rification de la condition de victoire
            if game.isWon(prnt=False):
                if verbose:
                    print(f"\n{'='*70}")
                    print(f"üçï PIZZA TROUV√âE EN {steps} √âTAPES!")
                    print(f"R√©compense totale: {total_reward:.1f}")
                    print(f"Cases visit√©es: {len(self.visited_cells)}")
                    print(f"Cases parcourues: {len(self.episode_path)}")
                
                # Visualisation du chemin
                if show_gui:
                    self.visualize_path(game, self.episode_path)
                
                # V√©rification de la performance optimale
                is_optimal = self.is_optimal_performance(steps, total_reward)
                if is_optimal and verbose:
                    print(f"üèÜ PERFORMANCE OPTIMALE ATTEINTE! ({steps} √©tapes)")
                
                print(f"{'='*70}")
                break
            
            # Transition vers l'√©tat suivant
            current_state = next_state
        
        # D√©croissance de l'exploration
        self.decay_exploration()
        
        success = True  # Toujours vrai car on continue jusqu'√† la victoire
        is_optimal = self.is_optimal_performance(steps, total_reward)
        
        # Enregistrement des performances
        self.performance_history.append({
            'episode': self.episode_count,
            'steps': steps,
            'reward': total_reward,
            'success': success,
            'optimal': is_optimal,
            'epsilon': self.epsilon,
            'path_length': len(self.episode_path),
            'unique_cells': len(self.visited_cells)
        })
        
        self.episode_count += 1
        
        if verbose:
            print(f"\nüìä R√©sum√© de l'√©pisode {self.episode_count}:")
            print(f"  - Succ√®s: Oui (pizza trouv√©e)")
            print(f"  - √âtapes: {steps}")
            print(f"  - R√©compense totale: {total_reward:.1f}")
            print(f"  - Performance optimale: {'Oui' if is_optimal else 'Non'}")
            print(f"  - Epsilon: {self.epsilon:.4f}")
            print(f"  - Meilleur score: {self.best_steps} √©tapes")
            print(f"  - Cases uniques visit√©es: {len(self.visited_cells)}")
            print(f"  - Longueur du chemin: {len(self.episode_path)}")
            print("=" * 70)
        
        return total_reward, steps, success, is_optimal
    
    def get_statistics(self) -> Dict[str, any]:
        """
        Retourne les statistiques de l'agent
        
        Returns:
            Dictionnaire contenant les statistiques
        """
        successful_episodes = [p for p in self.performance_history if p['success']]
        optimal_episodes = [p for p in self.performance_history if p['optimal']]
        
        return {
            'q_table_size': len(self.q_table),
            'total_state_actions': sum(len(actions) for actions in self.q_table.values()),
            'epsilon': self.epsilon,
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'episode_count': self.episode_count,
            'success_rate': len(successful_episodes) / max(self.episode_count, 1),
            'optimal_rate': len(optimal_episodes) / max(self.episode_count, 1),
            'best_steps': self.best_steps,
            'best_reward': self.best_reward,
            'consecutive_optimal': self.consecutive_optimal
        }
    
    def save(self, filename: str):
        """
        Sauvegarde l'agent dans un fichier
        
        Args:
            filename: Nom du fichier de sauvegarde
        """
        agent_data = {
            'q_table': dict(self.q_table),
            'learning_rate': self.learning_rate,
            'discount_factor': self.discount_factor,
            'epsilon': self.epsilon,
            'epsilon_decay': self.epsilon_decay,
            'epsilon_min': self.epsilon_min,
            'best_steps': self.best_steps,
            'best_reward': self.best_reward,
            'episode_count': self.episode_count,
            'performance_history': self.performance_history
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(agent_data, f)
        
        print(f"\nüíæ Agent sauvegard√© dans '{filename}'")
    
    @classmethod
    def load(cls, filename: str) -> 'OptimizedQLearningAgent':
        """
        Charge un agent depuis un fichier
        
        Args:
            filename: Nom du fichier de sauvegarde
            
        Returns:
            Agent charg√©
        """
        with open(filename, 'rb') as f:
            agent_data = pickle.load(f)
        
        agent = cls(
            learning_rate=agent_data['learning_rate'],
            discount_factor=agent_data['discount_factor'],
            epsilon=agent_data['epsilon'],
            epsilon_decay=agent_data['epsilon_decay'],
            epsilon_min=agent_data['epsilon_min']
        )
        
        agent.q_table = defaultdict(lambda: defaultdict(float), agent_data['q_table'])
        agent.best_steps = agent_data['best_steps']
        agent.best_reward = agent_data['best_reward']
        agent.episode_count = agent_data['episode_count']
        agent.performance_history = agent_data['performance_history']
        
        print(f"\nüìÇ Agent charg√© depuis '{filename}'")
        
        return agent


def train_until_optimal(
    environment_name: str = "maze",
    max_episodes: int = 1000,
    show_gui: bool = True,
    verbose: bool = True,
    target_optimal_episodes: int = 10,
    save_agent: bool = False
) -> OptimizedQLearningAgent:
    """
    Entra√Æne l'agent jusqu'√† atteindre une performance optimale
    
    Args:
        environment_name: Nom de l'environnement
        max_episodes: Nombre maximum d'√©pisodes
        show_gui: Afficher l'interface graphique
        verbose: Afficher les d√©tails de chaque √©tape
        target_optimal_episodes: Nombre d'√©pisodes optimaux cons√©cutifs requis
        save_agent: Sauvegarder l'agent apr√®s l'entra√Ænement
        
    Returns:
        Agent entra√Æn√©
    """
    agent = OptimizedQLearningAgent()
    agent.optimal_threshold = target_optimal_episodes
    
    print("=" * 80)
    print(f"ENTRA√éNEMENT Q-LEARNING OPTIMIS√â - DONATELLOPYZZA")
    print("=" * 80)
    print(f"Environnement: {environment_name}")
    print(f"Objectif: Trouver le chemin optimal vers la pizza")
    print(f"\nHyperparam√®tres:")
    print(f"  - Learning rate (Œ±): {agent.learning_rate}")
    print(f"  - Discount factor (Œ≥): {agent.discount_factor}")
    print(f"  - Epsilon initial (Œµ): {agent.epsilon}")
    print(f"  - √âpisodes max: {max_episodes}")
    print(f"  - Interface graphique: {'Oui' if show_gui else 'Non'}")
    print(f"  - Affichage verbose: {'Oui' if verbose else 'Non'}")
    print(f"  - D√©lai entre √©tapes: 0.01s (fixe)")
    print(f"  - √âpisodes optimaux requis: {target_optimal_episodes}")
    print(f"  - Limite d'√©tapes: Aucune (continue jusqu'√† la pizza)")
    print("=" * 80)
    
    start_time = time.time()
    optimal_episodes = 0
    consecutive_optimal = 0
    
    for episode in range(max_episodes):
        print(f"\nüöÄ √âPISODE {episode + 1}/{max_episodes}")
        print("=" * 70)
        
        game = RLGame(environment_name, gui=show_gui)
        turtle = game.start()
        
        reward, steps, success, is_optimal = agent.train_episode_optimized(
            game, turtle, show_gui, verbose
        )
        
        # Gestion des √©pisodes optimaux
        if is_optimal:
            optimal_episodes += 1
            consecutive_optimal += 1
            print(f"üèÜ √âPISODE OPTIMAL #{optimal_episodes} (cons√©cutif #{consecutive_optimal})")
        else:
            consecutive_optimal = 0
        
        # V√©rification de la condition d'arr√™t
        if consecutive_optimal >= target_optimal_episodes:
            print(f"\n{'='*80}")
            print(f"üéØ OBJECTIF ATTEINT!")
            print(f"L'agent a atteint une performance optimale pendant")
            print(f"{consecutive_optimal} √©pisodes cons√©cutifs!")
            print(f"{'='*80}")
            break
        
        # Affichage des statistiques p√©riodiques
        if (episode + 1) % 10 == 0:
            stats = agent.get_statistics()
            print(f"\nüìà Statistiques apr√®s {episode + 1} √©pisodes:")
            print(f"  - Taux de succ√®s: {stats['success_rate']:.1%}")
            print(f"  - Taux d'optimal: {stats['optimal_rate']:.1%}")
            print(f"  - Meilleur score: {stats['best_steps']} √©tapes")
            print(f"  - √âpisodes optimaux: {optimal_episodes}")
            print(f"  - Epsilon actuel: {stats['epsilon']:.4f}")
            print(f"  - Taille Q-table: {stats['q_table_size']} √©tats")
    
    training_time = time.time() - start_time
    
    print("\n" + "=" * 80)
    print(f"ENTRA√éNEMENT TERMIN√â")
    print("=" * 80)
    print(f"Dur√©e: {training_time:.1f}s")
    print(f"√âpisodes total: {agent.episode_count}")
    print(f"√âpisodes optimaux: {optimal_episodes}")
    
    stats = agent.get_statistics()
    print(f"\nStatistiques finales:")
    print(f"  - Taux de succ√®s: {stats['success_rate']:.1%}")
    print(f"  - Taux d'optimal: {stats['optimal_rate']:.1%}")
    print(f"  - Meilleur score: {stats['best_steps']} √©tapes")
    print(f"  - Taille Q-table: {stats['q_table_size']} √©tats")
    print(f"  - Epsilon final: {stats['epsilon']:.4f}")
    
    if consecutive_optimal >= target_optimal_episodes:
        print(f"\nüéâ SUCC√àS: Agent optimis√© en {agent.episode_count} √©pisodes!")
    else:
        print(f"\n‚ö†Ô∏è  ATTENTION: Objectif non atteint apr√®s {max_episodes} √©pisodes")
        print(f"   √âpisodes optimaux cons√©cutifs: {consecutive_optimal}/{target_optimal_episodes}")
    
    print("=" * 80)
    
    # Sauvegarde de l'agent si demand√©
    if save_agent:
        filename = f"qlearning_agent_{environment_name}_{int(time.time())}.pkl"
        agent.save(filename)
    
    return agent


def test_optimized_agent(
    agent: OptimizedQLearningAgent,
    environment_name: str = "maze",
    num_tests: int = 5,
    show_gui: bool = True,
    verbose: bool = True
) -> Tuple[float, List[Tuple[float, int, bool]]]:
    """
    Teste l'agent optimis√©
    
    Args:
        agent: Agent √† tester
        environment_name: Nom de l'environnement
        num_tests: Nombre de tests √† effectuer
        show_gui: Afficher l'interface graphique
        verbose: Afficher les d√©tails de chaque √©tape
        
    Returns:
        Tuple (taux de succ√®s, r√©sultats des tests)
    """
    test_results: List[Tuple[float, int, bool]] = []
    success_count = 0
    
    print("\n" + "=" * 80)
    print(f"PHASE DE TEST DE L'AGENT OPTIMIS√â")
    print("=" * 80)
    
    # Sauvegarde de l'epsilon original
    original_epsilon = agent.epsilon
    agent.epsilon = 0.0  # Pas d'exploration pendant les tests
    
    for test_num in range(num_tests):
        print(f"\nüß™ TEST {test_num + 1}/{num_tests}")
        print("=" * 70)
        
        game = RLGame(environment_name, gui=show_gui)
        turtle = game.start()
        
        reward, steps, success, is_optimal = agent.train_episode_optimized(
            game, turtle, show_gui, verbose
        )
        test_results.append((reward, steps, success))
        
        if success:
            success_count += 1
        
        status = "‚úÖ Succ√®s" if success else "‚ùå √âchec"
        optimal_status = "üèÜ Optimal" if is_optimal else "üìä Standard"
        print(f"\nR√©sultat du test {test_num + 1}: {status} | {optimal_status}")
        print(f"  - √âtapes: {steps}")
        print(f"  - R√©compense: {reward:.1f}")
    
    # Restauration de l'epsilon
    agent.epsilon = original_epsilon
    
    success_rate = success_count / num_tests
    
    print("\n" + "=" * 80)
    print(f"R√âSULTATS DES TESTS")
    print("=" * 80)
    print(f"Taux de succ√®s: {success_rate:.1%} ({success_count}/{num_tests})")
    if success_count > 0:
        avg_steps = sum(r[1] for r in test_results if r[2]) / success_count
        print(f"√âtapes moyennes (succ√®s): {avg_steps:.1f}")
        print(f"Meilleur score: {min(r[1] for r in test_results if r[2])} √©tapes")
    print("=" * 80)
    
    return success_rate, test_results


def main():
    """Point d'entr√©e principal du programme"""
    print("=" * 80)
    print("Q-LEARNING OPTIMIS√â POUR DONATELLOPYZZA")
    print("=" * 80)
    print("Objectif: Entra√Æner la tortue jusqu'√† trouver le chemin optimal")
    print("(le moins de cases possible vers la pizza)")
    
    environments = ["maze", "assessment_maze", "hard_maze", "line", "test"]
    
    print("\nEnvironnements disponibles:")
    for i, env in enumerate(environments, 1):
        print(f"  {i}. {env}")
    
    try:
        choice = int(input("\nChoisissez un environnement (1-5) [d√©faut: 1]: ") or "1")
        environment_name = environments[choice - 1] if 1 <= choice <= len(environments) else "maze"
    except (ValueError, IndexError):
        environment_name = "maze"
        print(f"Utilisation de l'environnement par d√©faut: {environment_name}")
    
    try:
        max_episodes = int(input("Nombre maximum d'√©pisodes [d√©faut: 500]: ") or "500")
        target_optimal = int(input("√âpisodes optimaux cons√©cutifs requis [d√©faut: 5]: ") or "5")
        show_gui = input("Afficher l'interface graphique ? (o/n) [d√©faut: o]: ").lower() != 'n'
        verbose = input("Affichage verbose (d√©tails de chaque √©tape) ? (o/n) [d√©faut: o]: ").lower() != 'n'
        save_agent = input("Sauvegarder l'agent apr√®s l'entra√Ænement ? (o/n) [d√©faut: n]: ").lower() == 'o'
    except ValueError:
        max_episodes = 500
        target_optimal = 5
        show_gui = True
        verbose = True
        save_agent = False
    
    print(f"\n‚öôÔ∏è  Configuration:")
    print(f"  - Environnement: {environment_name}")
    print(f"  - √âpisodes max: {max_episodes}")
    print(f"  - √âpisodes optimaux requis: {target_optimal}")
    print(f"  - Interface graphique: {'Oui' if show_gui else 'Non'}")
    print(f"  - Mode verbose: {'Oui' if verbose else 'Non'}")
    print(f"  - Sauvegarde: {'Oui' if save_agent else 'Non'}")
    print(f"  - D√©lai entre √©tapes: 0.01s (fixe)")
    print(f"  - Limite d'√©tapes: Aucune (continue jusqu'√† la pizza)")
    
    # Entra√Ænement optimis√©
    agent = train_until_optimal(
        environment_name=environment_name,
        max_episodes=max_episodes,
        show_gui=show_gui,
        verbose=verbose,
        target_optimal_episodes=target_optimal,
        save_agent=save_agent
    )
    
    # Tests de l'agent optimis√©
    test_choice = input("\nEffectuer des tests de l'agent ? (o/n) [d√©faut: o]: ").lower() != 'n'
    
    if test_choice:
        num_tests = int(input("Nombre de tests [d√©faut: 3]: ") or "3")
        test_verbose = input("Affichage verbose pour les tests ? (o/n) [d√©faut: n]: ").lower() == 'o'
        
        success_rate, test_results = test_optimized_agent(
            agent=agent,
            environment_name=environment_name,
            num_tests=num_tests,
            show_gui=show_gui,
            verbose=test_verbose
        )
    
    print("\nüéâ Programme termin√©!")
    print(f"Agent final avec {agent.get_statistics()['q_table_size']} √©tats appris")
    print(f"Meilleur score atteint: {agent.get_statistics()['best_steps']} √©tapes")


if __name__ == "__main__":
    main()
