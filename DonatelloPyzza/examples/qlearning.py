import sys
import os
import random
import time
import signal
from typing import Dict, Tuple, List, Any
from collections import defaultdict

# Configuration du chemin d'acc√®s au module parent
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.abspath(os.path.join(current_dir, ".."))
sys.path.insert(0, parent_dir)

from donatellopyzza import RLGame, Action, Feedback

# Variable globale pour g√©rer l'interruption clavier
interrupted = False

def signal_handler(signum, frame):
    """Gestionnaire pour l'interruption clavier (Ctrl+C)"""
    global interrupted
    print("\n\nInterruption d√©tect√©e (Ctrl+C)")
    print("Arr√™t de l'entra√Ænement...")
    interrupted = True


class QLearningAgent:
    """
    Agent Q-Learning simplifi√© pour DonatelloPyzza avec convergence
    
    Fonctionnalit√©s:
        - Apprentissage par renforcement avec Q-table
        - √âquation de Bellman pour mise √† jour des valeurs Q
        - Exploration vs exploitation (epsilon-greedy)
        - Syst√®me de r√©compenses simplifi√©
        - Convergence automatique bas√©e sur la stabilit√© des performances
    """
    
    # Constantes pour les valeurs magiques
    MIN_SUCCESS_RATE = 0.8  # Taux de succ√®s minimum pour convergence
    Q_VALUE_MIN = -1000.0   # Valeur Q minimale
    Q_VALUE_MAX = 1000.0    # Valeur Q maximale
    EFFICIENT_STEPS_THRESHOLD = 30  # Seuil pour consid√©rer un chemin efficace

    def __init__(
        self,
        learning_rate: float = 0.1,
        discount_factor: float = 0.9,
        epsilon: float = 0.3,
        epsilon_decay: float = 0.995,
        epsilon_min: float = 0.01,
        convergence_window: int = 20,
        convergence_threshold: float = 0.05,
        max_steps: int = 1000,
        random_seed: int = None,
        # Smart Explorer parameters
        systematic_exploration_episodes: int = 50,
        min_adaptive_steps: int = 50,
        adaptive_margin: float = 0.1
    ):
        """
        Initialise l'agent Q-Learning avec crit√®res de convergence et Smart Explorer
        
        Args:
            learning_rate: Taux d'apprentissage (Œ±)
            discount_factor: Facteur de r√©duction (Œ≥)
            epsilon: Taux d'exploration initial
            epsilon_decay: Facteur de d√©croissance d'epsilon
            epsilon_min: Valeur minimale d'epsilon
            convergence_window: Nombre d'√©pisodes pour √©valuer la convergence
            convergence_threshold: Seuil de variation pour consid√©rer la convergence
            max_steps: Nombre maximum d'√©tapes par √©pisode pour √©viter les boucles infinies
            systematic_exploration_episodes: Nombre d'√©pisodes pour exploration syst√©matique
            min_adaptive_steps: Nombre minimum d'√©tapes pour l'adaptation
            adaptive_margin: Marge pour accepter une l√©g√®re augmentation temporaire
        """
        # Hyperparam√®tres Q-Learning
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # Param√®tres de convergence
        self.convergence_window = convergence_window
        self.convergence_threshold = convergence_threshold
        self.max_steps = max_steps
        self.original_max_steps = max_steps  # Sauvegarde de la valeur initiale

        # Smart Explorer parameters
        self.systematic_exploration_episodes = systematic_exploration_episodes
        self.min_adaptive_steps = min_adaptive_steps
        self.adaptive_margin = adaptive_margin
        
        # Adaptive Episode Length
        self.best_successful_steps = float('inf')
        self.adaptive_steps_history = []
        self.adaptive_window = 10  # Nombre d'√©pisodes pour √©valuer l'adaptation

        # Q-table: √©tat -> {action: valeur_Q}
        self.q_table: Dict[Tuple, Dict[int, float]] = defaultdict(lambda: defaultdict(float))

        # Actions disponibles
        self.actions = [Action.MOVE_FORWARD, Action.TURN_LEFT, Action.TURN_RIGHT, Action.TOUCH]

        # Configuration des r√©compenses normalis√©es
        self.rewards = {
            'pizza_found': 100.0,      # R√©compense principale
            'pizza_touched': 50.0,     # R√©compense pour toucher la pizza
            'collision': -10.0,         # P√©nalit√© pour collision
            'step': -2.0,               # Co√ªt par √©tape (augment√© pour d√©courager les mouvements inutiles)
            'wall_touched': -5.0,      # P√©nalit√© pour toucher un mur
            'new_state': 3.0,          # Bonus pour d√©couvrir un nouvel √©tat (r√©duit)
            'redundant_action': -3.0   # P√©nalit√© pour actions redondantes
        }

        # Statistiques et suivi de convergence
        self.episode_count = 0
        self.best_steps = float('inf')
        self.episode_results = []  # Historique des r√©sultats pour convergence
        self.converged = False
        
        # Suivi des √©tats visit√©s pour bonus d'exploration
        self.visited_states = set()
        
        # Position de la pizza (sera d√©tect√©e dynamiquement)
        self.pizza_position = None
        
        # Historique des actions pour d√©tecter les mouvements redondants
        self.action_history = []
        self.max_history = 3  # Garder les 3 derni√®res actions
        
        # Initialisation du seed al√©atoire pour reproductibilit√©
        if random_seed is not None:
            random.seed(random_seed)

    def get_state(self, position: Tuple[int, int], orientation: int, feedback: Feedback = None) -> Tuple[int, int, int]:
        """
        G√©n√®re une repr√©sentation d'√©tat unique sous forme de tuple hashable
        
        Args:
            position: Position actuelle (x, y)
            orientation: Orientation actuelle (0-3)
            feedback: Dernier feedback re√ßu (non utilis√© pour simplifier l'espace d'√©tats)
            
        Returns:
            Tuple repr√©sentant l'√©tat (x, y, orientation)
        """
        # √âtat simplifi√© sans feedback pour r√©duire l'espace d'√©tats
        # Le feedback est une cons√©quence de l'action, pas une propri√©t√© de l'√©tat
        return (position[0], position[1], orientation)

    def calculate_reward(self, feedback: Feedback, state: Tuple, position: Tuple[int, int], action: Action = None) -> float:
        """
        Calcule la r√©compense bas√©e sur le feedback avec bonus d'exploration et d√©tection de redondance
        
        Args:
            feedback: Feedback re√ßu de l'environnement
            state: √âtat actuel
            position: Position actuelle
            action: Action effectu√©e
            
        Returns:
            Valeur de la r√©compense
        """
        base_reward = 0.0
        
        # R√©compenses de base
        if feedback == Feedback.MOVED_ON_PIZZA:
            base_reward = self.rewards['pizza_found']
        elif feedback == Feedback.TOUCHED_PIZZA:
            base_reward = self.rewards['pizza_touched']
        elif feedback == Feedback.COLLISION:
            base_reward = self.rewards['collision']
        elif feedback == Feedback.TOUCHED_WALL:
            base_reward = self.rewards['wall_touched']
        else:
            base_reward = self.rewards['step']
        
        # Bonus d'exploration pour nouvel √©tat
        if state not in self.visited_states:
            base_reward += self.rewards['new_state']
            self.visited_states.add(state)
        
        # D√©tection d'actions redondantes
        if action is not None:
            self.action_history.append(action)
            if len(self.action_history) > self.max_history:
                self.action_history.pop(0)
            
            # D√©tecter les patterns redondants
            if len(self.action_history) >= 2:
                # TURN_RIGHT suivi de TURN_LEFT (ou vice versa) = mouvement inutile
                if (self.action_history[-2] == Action.TURN_RIGHT and self.action_history[-1] == Action.TURN_LEFT) or \
                   (self.action_history[-2] == Action.TURN_LEFT and self.action_history[-1] == Action.TURN_RIGHT):
                    base_reward += self.rewards['redundant_action']
                
                # M√™me action r√©p√©t√©e plusieurs fois sans mouvement
                if len(self.action_history) >= 3 and \
                   self.action_history[-3] == self.action_history[-2] == self.action_history[-1] and \
                   self.action_history[-1] in [Action.TURN_LEFT, Action.TURN_RIGHT]:
                    base_reward += self.rewards['redundant_action']
        
        return base_reward

    def systematic_exploration(self, state: Tuple) -> Action:
        """
        Exploration syst√©matique : tour d'horizon complet avant de se d√©placer
        
        Args:
            state: √âtat actuel (position, orientation)
            
        Returns:
            Action d'exploration syst√©matique
        """
        # Si nouvel √©tat, faire un tour d'horizon complet
        if state not in self.visited_states:
            # S√©quence d'exploration : TOUCH pour d√©tecter les obstacles
            # puis tourner pour explorer toutes les directions
            if not hasattr(self, '_exploration_sequence'):
                self._exploration_sequence = [
                    Action.TOUCH,      # D√©tecter obstacle devant
                    Action.TURN_LEFT,  # Regarder √† gauche
                    Action.TOUCH,      # D√©tecter obstacle √† gauche
                    Action.TURN_LEFT,  # Regarder en arri√®re
                    Action.TOUCH,      # D√©tecter obstacle derri√®re
                    Action.TURN_LEFT,   # Regarder √† droite
                    Action.TOUCH,      # D√©tecter obstacle √† droite
                    Action.TURN_LEFT,  # Retourner face avant
                    Action.MOVE_FORWARD # Avancer d'un pas
                ]
                self._exploration_index = 0
            
            # Ex√©cuter la s√©quence d'exploration
            if self._exploration_index < len(self._exploration_sequence):
                action = self._exploration_sequence[self._exploration_index]
                self._exploration_index += 1
                return action
            else:
                # S√©quence termin√©e, r√©initialiser pour le prochain √©tat
                self._exploration_index = 0
                return Action.MOVE_FORWARD
        
        # Si √©tat d√©j√† visit√©, utiliser Q-Learning classique
        return self.choose_action_classic(state)

    def choose_action_classic(self, state: Tuple) -> Action:
        """
        S√©lection d'action classique (Q-Learning standard)
        
        Args:
            state: √âtat actuel
            
        Returns:
            Action √† ex√©cuter
        """
        # Exploration adaptative: moins d'exploration si l'agent performe bien
        exploration_rate = self.epsilon
        if self.best_steps < 20 and self.episode_count > 50:  # Si l'agent a d√©j√† trouv√© un bon chemin
            exploration_rate *= 0.5  # R√©duire l'exploration de moiti√©
        
        # Exploration: action al√©atoire
        if random.random() < exploration_rate:
            return random.choice(self.actions)

        # Exploitation: meilleure action connue
        if state in self.q_table and self.q_table[state]:
            best_action_key = max(self.q_table[state], key=self.q_table[state].get)
            # Conversion directe de la cl√© vers l'Action
            return Action(best_action_key)

        # Si aucune information, action al√©atoire
        return random.choice(self.actions)

    def choose_action(self, state: Tuple) -> Action:
        """
        Smart Explorer : S√©lection d'action intelligente avec exploration syst√©matique
        
        Args:
            state: √âtat actuel
            
        Returns:
            Action √† ex√©cuter
        """
        # Phase 1: Exploration syst√©matique pour les premiers √©pisodes
        if self.episode_count < self.systematic_exploration_episodes:
            return self.systematic_exploration(state)
        
        # Phase 2: Q-Learning classique avec exploration adaptative
        else:
            return self.choose_action_classic(state)

    def update_q_value(self, state: Tuple, action: Action, reward: float, next_state: Tuple):
        """
        Met √† jour la Q-table selon l'√©quation de Bellman avec validation
        
        Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥ max Q(s',a') - Q(s,a)]
        
        Args:
            state: √âtat actuel
            action: Action effectu√©e
            reward: R√©compense re√ßue
            next_state: Nouvel √©tat
        """
        action_key = int(action.value)
        current_q = self.q_table[state][action_key]

        # Calcul de la valeur maximale du prochain √©tat
        if next_state in self.q_table and self.q_table[next_state]:
            max_next_q = max(self.q_table[next_state].values())
        else:
            max_next_q = 0.0

        # √âquation de Bellman
        td_target = reward + self.discount_factor * max_next_q
        td_error = td_target - current_q
        new_q = current_q + self.learning_rate * td_error

        # Validation et clipping des valeurs Q
        if not isinstance(new_q, (int, float)) or new_q != new_q:  # NaN check
            new_q = 0.0
        elif new_q == float('inf') or new_q == float('-inf'):
            new_q = 0.0
        else:
            # Clipping dans une plage raisonnable
            new_q = max(self.Q_VALUE_MIN, min(self.Q_VALUE_MAX, new_q))

        self.q_table[state][action_key] = new_q

    def update_adaptive_episode_length(self, steps: int, success: bool):
        """
        Met √† jour la limite d'√©pisodes adaptative bas√©e sur les performances
        
        Args:
            steps: Nombre d'√©tapes de l'√©pisode
            success: Si l'√©pisode a r√©ussi
        """
        if success:
            # Mettre √† jour le meilleur score r√©ussi
            if steps < self.best_successful_steps:
                self.best_successful_steps = steps
                # Ajuster max_steps avec une marge de s√©curit√©
                new_max_steps = max(
                    int(steps * (1 + self.adaptive_margin)),  # Marge de 10% par d√©faut
                    self.min_adaptive_steps  # Minimum absolu
                )
                self.max_steps = new_max_steps
                
                if hasattr(self, '_verbose_adaptive') and self._verbose_adaptive:
                    print(f"  üîÑ Adaptive Episode Length: max_steps ajust√© √† {self.max_steps} "
                          f"(meilleur score: {steps} √©tapes)")
            
            # Enregistrer dans l'historique pour analyse
            self.adaptive_steps_history.append(steps)
            if len(self.adaptive_steps_history) > self.adaptive_window:
                self.adaptive_steps_history.pop(0)
            
            # V√©rifier si on peut optimiser davantage
            if len(self.adaptive_steps_history) >= 5:
                recent_avg = sum(self.adaptive_steps_history[-5:]) / 5
                if recent_avg < self.max_steps * 0.8:  # Si on fait 20% mieux que la limite
                    new_limit = max(int(recent_avg * 1.1), self.min_adaptive_steps)
                    if new_limit < self.max_steps:
                        self.max_steps = new_limit
                        if hasattr(self, '_verbose_adaptive') and self._verbose_adaptive:
                            print(f"  üìà Optimisation: max_steps r√©duit √† {self.max_steps} "
                                  f"(moyenne r√©cente: {recent_avg:.1f})")

    def decay_epsilon(self):
        """R√©duit le taux d'exploration de mani√®re standard"""
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

    def check_convergence(self) -> bool:
        """
        V√©rifie si l'agent a converg√© bas√© sur la stabilit√© des performances
        
        Returns:
            True si l'agent a converg√©, False sinon
        """
        # Pas assez de donn√©es pour √©valuer la convergence
        if len(self.episode_results) < self.convergence_window:
            return False
        
        # R√©cup√®re les derniers r√©sultats
        recent_results = self.episode_results[-self.convergence_window:]
        
        # Calcule les statistiques de performance
        stats = self._calculate_performance_stats(recent_results)
        
        # Calcul du taux de succ√®s r√©cent
        recent_successes = sum(1 for result in recent_results if result[2])  # result[2] = success
        success_rate = recent_successes / len(recent_results)
        
        # Crit√®res de convergence am√©lior√©s
        convergence_criteria = [
            stats['cv'] < self.convergence_threshold,  # Faible variation des performances
            self.epsilon <= self.epsilon_min,  # Exploration minimale atteinte
            len(self.episode_results) >= self.convergence_window,  # Suffisamment d'√©pisodes
            success_rate >= self.MIN_SUCCESS_RATE  # Taux de succ√®s minimum
        ]
        
        # Convergence si tous les crit√®res sont remplis
        converged = all(convergence_criteria)
        
        if converged:
            self.converged = True
            
        return converged

    def _execute_step(self, game: RLGame, turtle, current_state: Tuple, steps: int, verbose: bool) -> Tuple[float, Tuple, bool]:
        """
        Ex√©cute un pas d'apprentissage
        
        Args:
            game: Instance du jeu
            turtle: Tortue √† contr√¥ler
            current_state: √âtat actuel
            steps: Num√©ro de l'√©tape actuelle (pour l'affichage)
            verbose: Affichage d√©taill√©
            
        Returns:
            Tuple (r√©compense, nouvel √©tat, succ√®s)
        """
        # S√©lection et ex√©cution de l'action
        action = self.choose_action(current_state)
        feedback, _ = turtle.execute(action)

        # Nouvel √©tat
        new_position = game.getTurtlePosition(turtle)
        new_orientation = game.getTurtleOrientation(turtle)
        next_state = self.get_state(new_position, new_orientation, feedback)

        # Calcul de la r√©compense
        reward = self.calculate_reward(feedback, current_state, new_position, action)
        
        # D√©tection de la position de la pizza
        if feedback == Feedback.MOVED_ON_PIZZA and self.pizza_position is None:
            self.pizza_position = new_position

        # Mise √† jour de la Q-table
        self.update_q_value(current_state, action, reward, next_state)

        # Affichage
        if verbose:
            print(f"Episode {self.episode_count + 1}, √©tape {steps}: {str(action)} -> "
                  f"pos {new_position}, r√©compense {reward:.1f}, {str(feedback)}")

        return reward, next_state, game.isWon(prnt=False)

    def _update_statistics(self, total_reward: float, steps: int, success: bool):
        """
        Met √† jour les statistiques de l'agent avec Adaptive Episode Length
        
        Args:
            total_reward: R√©compense totale de l'√©pisode
            steps: Nombre d'√©tapes
            success: Succ√®s de l'√©pisode
        """
        # Mise √† jour des meilleures performances
        if success and steps < self.best_steps:
            self.best_steps = steps

        # Enregistrement des r√©sultats pour l'analyse de convergence
        self.episode_results.append((total_reward, steps, success))
        
        # Adaptive Episode Length: ajuster max_steps bas√© sur les performances
        self.update_adaptive_episode_length(steps, success)
        
        # D√©croissance de l'exploration
        self.decay_epsilon()
        
        # R√©initialisation de l'historique des actions pour le prochain √©pisode
        self.action_history.clear()
        
        # visited_states n'est plus r√©initialis√© pour r√©compenser la vraie d√©couverte

    def _calculate_performance_stats(self, results: List[Tuple]) -> Dict[str, float]:
        """
        Calcule les statistiques de performance
        
        Args:
            results: Liste de tuples (reward, steps, success)
            
        Returns:
            Dictionnaire avec mean, std_dev, variance, cv
        """
        scores = [result[1] for result in results]
        mean_score = sum(scores) / len(scores)
        variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)
        std_dev = variance ** 0.5
        coefficient_variation = std_dev / mean_score if mean_score > 0 else float('inf')
        
        return {
            'mean': mean_score,
            'std_dev': std_dev,
            'variance': variance,
            'cv': coefficient_variation
        }

    def get_convergence_info(self) -> Dict[str, Any]:
        """
        Retourne les informations sur l'√©tat de convergence
        
        Returns:
            Dictionnaire avec les informations de convergence
        """
        if len(self.episode_results) < self.convergence_window:
            return {
                'converged': False,
                'episodes_needed': self.convergence_window - len(self.episode_results),
                'reason': 'Pas assez d\'√©pisodes',
                'epsilon': self.epsilon
            }
        
        recent_results = self.episode_results[-self.convergence_window:]
        stats = self._calculate_performance_stats(recent_results)
        
        return {
            'converged': self.converged,
            'coefficient_variation': stats['cv'],
            'threshold': self.convergence_threshold,
            'mean_performance': stats['mean'],
            'std_performance': stats['std_dev'],
            'epsilon': self.epsilon,
            'episodes_analyzed': len(recent_results)
        }

    def train_episode(self, game: RLGame, turtle, show_gui: bool = True, verbose: bool = True, training_mode: bool = True) -> Tuple[float, int, bool]:
        """
        Ex√©cute un √©pisode (entra√Ænement ou test selon training_mode)
        
        Args:
            game: Instance du jeu
            turtle: Tortue √† contr√¥ler
            show_gui: Afficher l'interface graphique
            verbose: Afficher les d√©tails
            training_mode: Si True, met √† jour les statistiques et v√©rifie la convergence
            
        Returns:
            Tuple (r√©compense totale, nombre d'√©tapes, succ√®s)
        """
        total_reward = 0.0
        steps = 0

        # √âtat initial
        current_position = game.getTurtlePosition(turtle)
        current_orientation = game.getTurtleOrientation(turtle)
        current_state = self.get_state(current_position, current_orientation)

        # Boucle principale - Continue jusqu'√† la victoire ou limite d'√©tapes
        while steps < self.max_steps:
            steps += 1

            # Ex√©cution d'un pas d'apprentissage
            reward, next_state, success = self._execute_step(game, turtle, current_state, steps, verbose)
            total_reward += reward

            # D√©lai pour la visualisation
            if show_gui:
                time.sleep(0.01)

            # V√©rification de la victoire
            if success:
                if verbose:
                    print(f"\nVictoire! Pizza trouv√©e en {steps} √©tapes (r√©compense: {total_reward:.1f})")
                break

            # Transition vers l'√©tat suivant
            current_state = next_state
        
        # V√©rification si l'√©pisode a √©t√© interrompu par la limite d'√©tapes
        if steps >= self.max_steps and not success:
            if verbose:
                print(f"\nLimite d'√©tapes atteinte ({self.max_steps}), √©pisode interrompu (r√©compense: {total_reward:.1f})")

        # Mise √† jour des statistiques seulement en mode entra√Ænement
        if training_mode:
            self._update_statistics(total_reward, steps, success)
            self.episode_count += 1
            
            # V√©rification de la convergence
            converged = self.check_convergence()
        else:
            converged = False

        if verbose:
            new_states_count = len(self.visited_states)
            efficiency = "efficace" if success and steps <= self.EFFICIENT_STEPS_THRESHOLD else "normal"
            convergence_status = "converg√©" if converged else "en cours"
            
            # Affichage des informations adaptatives
            adaptive_info = ""
            if hasattr(self, 'best_successful_steps') and self.best_successful_steps != float('inf'):
                adaptive_info = f", limite adaptative: {self.max_steps} (meilleur: {self.best_successful_steps})"
            
            print(f"\nEpisode {self.episode_count}: "
                  f"{'Succ√®s' if success else '√âchec'} en {steps} √©tapes, "
                  f"r√©compense {total_reward:.1f}, "
                  f"epsilon {self.epsilon:.3f}, "
                  f"{new_states_count} nouveaux √©tats, "
                  f"performance {efficiency}, "
                  f"convergence {convergence_status}{adaptive_info}")

        return total_reward, steps, success


    def get_statistics(self) -> Dict[str, any]:
        """Retourne les statistiques de l'agent"""
        return {
            'q_table_size': len(self.q_table),
            'epsilon': self.epsilon,
            'episode_count': self.episode_count,
            'best_steps': self.best_steps,
            'converged': self.converged
        }

    def print_convergence_summary(self):
        """Affiche un r√©sum√© d√©taill√© de l'√©tat de convergence"""
        convergence_info = self.get_convergence_info()
        
        print("\n" + "=" * 50)
        print("R√âSUM√â DE CONVERGENCE")
        print("=" * 50)
        
        if convergence_info['converged']:
            print("CONVERGENCE ATTEINTE!")
            print(f"   - Coefficient de variation: {convergence_info['coefficient_variation']:.4f}")
            print(f"   - Seuil requis: {convergence_info['threshold']}")
            print(f"   - Performance moyenne: {convergence_info['mean_performance']:.1f} √©tapes")
            print(f"   - √âcart-type: {convergence_info['std_performance']:.1f} √©tapes")
            print(f"   - √âpisodes analys√©s: {convergence_info['episodes_analyzed']}")
        else:
            print("Convergence en cours...")
            if 'episodes_needed' in convergence_info:
                print(f"   - √âpisodes n√©cessaires: {convergence_info['episodes_needed']}")
            else:
                print(f"   - CV actuel: {convergence_info['coefficient_variation']:.4f}")
                print(f"   - Seuil requis: {convergence_info['threshold']}")
                print(f"   - Performance moyenne: {convergence_info['mean_performance']:.1f} √©tapes")
                print(f"   - √âpisodes analys√©s: {convergence_info['episodes_analyzed']}")
        
        print(f"   - Epsilon actuel: {convergence_info['epsilon']:.3f}")
        print("=" * 50)


def train_agent(
    environment_name: str = "maze",
    show_gui: bool = True,
    verbose: bool = True
) -> QLearningAgent:
    """
    Entra√Æne l'agent Q-Learning jusqu'√† convergence
    
    Les param√®tres de convergence sont g√©r√©s automatiquement par l'agent.
    L'agent s'arr√™te uniquement sur convergence, sans limite d'√©pisodes.
    Supporte l'interruption clavier (Ctrl+C) pour arr√™t propre.
    
    Args:
        environment_name: Nom de l'environnement
        show_gui: Afficher l'interface graphique
        verbose: Afficher les d√©tails
        
    Returns:
        Agent entra√Æn√©
    """
    global interrupted
    interrupted = False
    
    # Configuration du gestionnaire de signal pour Ctrl+C
    signal.signal(signal.SIGINT, signal_handler)
    
    agent = QLearningAgent()
    
    # Activer le mode verbose pour l'adaptation
    agent._verbose_adaptive = verbose

    print("=" * 60)
    print("ENTRA√éNEMENT Q-LEARNING AVEC SMART EXPLORER - DONATELLOPYZZA")
    print("=" * 60)
    print(f"Environnement: {environment_name}")
    print(f"Fen√™tre de convergence: {agent.convergence_window} √©pisodes (automatique)")
    print(f"Seuil de convergence: {agent.convergence_threshold} (automatique)")
    print(f"Limite d'√©tapes initiale: {agent.max_steps} (adaptative)")
    print(f"Learning rate: {agent.learning_rate}")
    print(f"Discount factor: {agent.discount_factor}")
    print(f"Epsilon initial: {agent.epsilon}")
    print(f"Smart Explorer: {agent.systematic_exploration_episodes} √©pisodes d'exploration syst√©matique")
    print(f"Adaptive Episode Length: limite minimale {agent.min_adaptive_steps} √©tapes")
    print("=" * 60)
    print("Nouvelles fonctionnalit√©s:")
    print("  üîç Smart Explorer: Exploration syst√©matique pour nouveaux √©tats")
    print("  üìè Adaptive Episode Length: Ajustement automatique des limites")
    print("  üéØ Convergence intelligente: Optimisation bas√©e sur les performances")
    print("=" * 60)
    print("Astuce: Appuyez sur Ctrl+C pour arr√™ter l'entra√Ænement √† tout moment")
    print("=" * 60)

    successful_episodes = 0
    episode = 0

    # Boucle d'entra√Ænement bas√©e sur la convergence uniquement
    while not agent.converged and not interrupted:
        episode += 1
        print(f"\n√âPISODE {episode}")
        print("-" * 40)

        game = RLGame(environment_name, gui=show_gui)
        turtle = game.start()

        reward, steps, success = agent.train_episode(game, turtle, show_gui, verbose)

        if success:
            successful_episodes += 1

        # Affichage des statistiques p√©riodiques am√©lior√©es
        if episode % 10 == 0:
            stats = agent.get_statistics()
            success_rate = successful_episodes / episode
            
            print(f"\nStatistiques apr√®s {episode} √©pisodes:")
            print(f"  Taux de succ√®s global: {success_rate:.1%}")
            
            # Ajout du taux de succ√®s r√©cent
            if len(agent.episode_results) >= agent.convergence_window:
                recent_results = agent.episode_results[-agent.convergence_window:]
                recent_successes = sum(1 for result in recent_results if result[2])
                recent_success_rate = recent_successes / len(recent_results)
                print(f"  Taux de succ√®s r√©cent ({agent.convergence_window}): {recent_success_rate:.1%}")
            
            print(f"  Meilleur chemin: {stats['best_steps']} √©tapes")
            print(f"  Epsilon: {stats['epsilon']:.3f}")
            print(f"  Q-table: {stats['q_table_size']} √©tats")
            
            if stats['best_steps'] < 50:
                print(f"  Performance: excellente (‚â§50 √©tapes)")
            elif stats['best_steps'] < 100:
                print(f"  Performance: bonne (‚â§100 √©tapes)")
            else:
                print(f"  Performance: en cours d'apprentissage")
            
            # Affichage du r√©sum√© de convergence
            agent.print_convergence_summary()

    # R√©sultats finaux
    print("\n" + "=" * 60)
    if interrupted:
        print("Entra√Ænement interrompu par l'utilisateur (Ctrl+C)")
    else:
        print("Entra√Ænement termin√© - convergence atteinte")
    print("=" * 60)
    
    final_stats = agent.get_statistics()
    final_success_rate = successful_episodes / episode
    convergence_info = agent.get_convergence_info()
    
    print(f"√âpisodes total: {episode}")
    print(f"Taux de succ√®s final: {final_success_rate:.1%}")
    print(f"Meilleur chemin trouv√©: {final_stats['best_steps']} √©tapes")
    print(f"√âtats appris: {final_stats['q_table_size']}")
    
    if convergence_info['converged']:
        print(f"Convergence: atteinte (CV: {convergence_info['coefficient_variation']:.4f})")
        print(f"Performance stable: {convergence_info['mean_performance']:.1f} ¬± {convergence_info['std_performance']:.1f} √©tapes")
    else:
        print("Convergence: non atteinte")
    
    # √âvaluation de la performance finale
    if final_stats['best_steps'] <= 20:
        print("Performance exceptionnelle!")
    elif final_stats['best_steps'] <= 30:
        print("Performance excellente!")
    elif final_stats['best_steps'] <= 50:
        print("Performance tr√®s bonne!")
    elif final_stats['best_steps'] <= 100:
        print("Performance bonne!")
    else:
        print("Performance en cours d'am√©lioration!")
    
    print("=" * 60)
    
    # Affichage du r√©sum√© final de convergence
    agent.print_convergence_summary()

    return agent


def test_agent(
    agent: QLearningAgent,
    environment_name: str = "maze",
    num_tests: int = 5,
    show_gui: bool = True,
    verbose: bool = True
) -> Tuple[float, List[int]]:
    """
    Teste l'agent entra√Æn√©
    
    Args:
        agent: Agent √† tester
        environment_name: Nom de l'environnement
        num_tests: Nombre de tests
        show_gui: Afficher l'interface graphique
        verbose: Afficher les d√©tails
        
    Returns:
        Tuple (taux de succ√®s, liste des scores)
    """
    print("\n" + "=" * 60)
    print("PHASE DE TEST")
    print("=" * 60)

    # Sauvegarde de l'epsilon original
    original_epsilon = agent.epsilon
    agent.epsilon = 0.0  # Pas d'exploration pendant les tests

    test_results = []
    success_count = 0

    for test_num in range(num_tests):
        print(f"\nTEST {test_num + 1}/{num_tests}")
        print("-" * 30)

        game = RLGame(environment_name, gui=show_gui)
        turtle = game.start()

        # Utilisation du mode test (training_mode=False)
        reward, steps, success = agent.train_episode(game, turtle, show_gui, verbose, training_mode=False)
        test_results.append(steps)

        if success:
            success_count += 1

        status = "Succ√®s" if success else "√âchec"
        print(f"R√©sultat: {status} | {steps} √©tapes")

    # Restauration de l'epsilon
    agent.epsilon = original_epsilon

    success_rate = success_count / num_tests

    print("\n" + "=" * 60)
    print("R√âSULTATS DES TESTS")
    print("=" * 60)
    print(f"Taux de succ√®s: {success_rate:.1%} ({success_count}/{num_tests})")
    if success_count > 0:
        avg_steps = sum(test_results) / len(test_results)
        best_score = min(test_results)
        print(f"Score moyen: {avg_steps:.1f} √©tapes")
        print(f"Meilleur chemin: {best_score} √©tapes")
        
        # √âvaluation de la performance des tests
        if best_score <= 15:
            print("Tests: performance exceptionnelle!")
        elif best_score <= 25:
            print("Tests: performance excellente!")
        elif best_score <= 40:
            print("Tests: performance tr√®s bonne!")
        else:
            print("Tests: performance bonne!")
    print("=" * 60)

    return success_rate, test_results


def get_user_config() -> Dict[str, Any]:
    """
    Collecte la configuration utilisateur
    
    Returns:
        Dictionnaire avec la configuration
    """
    print("=" * 60)
    print("Q-LEARNING SIMPLIFI√â POUR DONATELLOPYZZA")
    print("=" * 60)
    print("Objectif: Apprendre √† naviguer vers la pizza")
    print("M√©thode: Apprentissage par renforcement (Q-Learning)")
    print("=" * 60)

    environments = ["maze", "assessment_maze", "hard_maze", "line", "test"]

    print("\nEnvironnements disponibles:")
    for i, env in enumerate(environments, 1):
        print(f"  {i}. {env}")

    # Validation de l'environnement avec boucle
    while True:
        try:
            choice = int(input("\nChoisissez un environnement (1-5) [d√©faut: 1]: ") or "1")
            if 1 <= choice <= len(environments):
                environment_name = environments[choice - 1]
                break
            else:
                print("Choix invalide. Veuillez choisir entre 1 et 5.")
        except ValueError:
            print("Entr√©e invalide. Veuillez entrer un nombre.")

    try:
        show_gui = input("Afficher l'interface graphique ? (o/n) [d√©faut: o]: ").lower() != 'n'
        verbose = input("Affichage d√©taill√© ? (o/n) [d√©faut: o]: ").lower() != 'n'
    except ValueError:
        show_gui = True
        verbose = True
    
    # Param√®tres de convergence fixes (g√©r√©s automatiquement par l'agent)
    convergence_window = 20
    convergence_threshold = 0.05

    return {
        'environment_name': environment_name,
        'show_gui': show_gui,
        'verbose': verbose
    }

def run_training_pipeline(config: Dict[str, Any]) -> QLearningAgent:
    """
    Ex√©cute le pipeline d'entra√Ænement complet
    
    Args:
        config: Configuration utilisateur
        
    Returns:
        Agent entra√Æn√©
    """
    # Entra√Ænement avec convergence
    agent = train_agent(
        environment_name=config['environment_name'],
        show_gui=config['show_gui'],
        verbose=config['verbose']
    )

    # Tests (seulement si l'entra√Ænement n'a pas √©t√© interrompu)
    if not interrupted:
        test_choice = input("\nEffectuer des tests ? (o/n) [d√©faut: o]: ").lower() != 'n'
        if test_choice:
            num_tests = int(input("Nombre de tests [d√©faut: 3]: ") or "3")
            test_verbose = input("Affichage d√©taill√© pour les tests ? (o/n) [d√©faut: n]: ").lower() == 'o'

            success_rate, test_results = test_agent(
                agent=agent,
                environment_name=config['environment_name'],
                num_tests=num_tests,
                show_gui=config['show_gui'],
                verbose=test_verbose
            )

    return agent

def main():
    """Point d'entr√©e principal"""
    config = get_user_config()
    agent = run_training_pipeline(config)
    
    print("\nProgramme termin√©!")
    print(f"Agent final avec {agent.get_statistics()['q_table_size']} √©tats appris")
    print(f"Chemin optimal: {agent.get_statistics()['best_steps']} √©tapes")


if __name__ == "__main__":
    main()
